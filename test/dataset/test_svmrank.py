import pickle
from io import BytesIO

# from nose.tools import assert_almost_equal
# from nose.tools import assert_equals
# from nose.tools import raises
from pytest import raises
from pytest import approx
from pytorchltr.dataset.svmrank import svmranking_dataset as load
from pytorchltr.dataset.svmrank import create_svmranking_collate_fn


dataset_txt = """0 qid:1 1:1.000000 2:1.000000 3:0.833333 4:0.871264 5:0 6:0 7:0 8:0.941842 9:1.000000 10:1.000000 11:1.000000 12:1.000000 13:1.000000 14:1.000000 15:1.000000 16:1.000000 17:1.000000 18:0.719697 19:0.729351 22:0 23:0.811565 24:1.000000 25:0.972730 26:1.000000 27:1.000000 28:0.922374 29:0.946654 30:0.938888 31:1.000000 32:1.000000 33:0.711276 34:0.722202 35:0 36:0 37:0 38:0.798002 39:1.000000 40:1.000000 41:1.000000 42:1.000000 43:0.959134 44:0.963919 45:0.971425 #docid = 244338
2 qid:1 1:0.600000 2:0.600000 3:1.000000 4:1.000000 5:0 6:0 7:0 8:1.000000 9:0.624834 10:0.767301 11:0.816099 12:0.934805 13:0.649685 14:0.680222 15:0.686762 16:0.421053 17:0.680904 18:1.000000 19:1.000000 22:0 23:1.000000 24:0.401391 25:0.938966 26:0.949446 27:0.984769 28:0.955266 29:1.000000 30:0.997786 31:0.441860 32:0.687033 33:1.000000 34:1.000000 35:0 36:0 37:0 38:1.000000 39:0.425450 40:0.975968 41:0.928785 42:0.978524 43:0.979553 44:1.000000 45:1.000000 #docid = 143821
0 qid:1 1:0.400000 2:0.400000 3:0.555555 4:0.563658 5:0 6:0 7:0 8:0.545844 9:0.380576 10:0.427356 11:0.468244 12:0.756579 13:0.366316 14:0.360838 15:0.373909 16:0.210526 17:0.479859 18:0.595237 19:0.608701 22:0 23:0.613865 24:0.184562 25:0.791539 26:0.863833 27:0.957024 28:0.896468 29:0.941132 30:0.946305 31:0.232558 32:0.507810 33:0.603068 34:0.616847 35:0 36:0 37:0 38:0.614004 39:0.202374 40:0.812801 41:0.868091 42:0.958879 43:0.926045 44:0.944576 45:0.963753 #docid = 285257
0 qid:1 1:0.200000 2:0.200000 3:0.277778 4:0.281830 5:0 6:0 7:0 8:0.330027 9:0.244258 10:0.303171 11:0.301165 12:0.614995 13:0.255036 14:0.188541 15:0.205136 16:0.368421 17:0.637735 18:0.795453 19:0.802720 22:0 23:0.851563 24:0.364249 25:0.897048 26:0.914052 27:0.973615 28:0.923365 29:0.954078 30:0.965119 31:0.348837 32:0.600470 33:0.711821 34:0.718938 35:0 36:0 37:0 38:0.773746 39:0.354506 40:0.871098 41:0.865835 42:0.958123 43:0.917441 44:0.918191 45:0.935256 #docid = 201684
1 qid:1 1:3.500000 2:0.300000 3:0.000000 4:0.000000 5:0 6:0 7:0 8:0.000000 9:0.000000 10:0.000000 11:0.000000 12:0.000000 13:0.000000 14:0.000000 15:0.000000 16:0.263158 17:0.592453 18:0.328947 19:0.340126 22:0 23:0.393538 24:0.258450 25:0.651875 26:0.885764 27:0.964385 28:0.976550 29:0.970655 30:0.984686 31:0.232558 32:0.546115 33:0.295381 34:0.306186 35:0 36:0 37:0 38:0.355037 39:0.230125 40:0.613531 41:0.860290 42:0.956255 43:0.973015 44:0.939784 45:0.960934 #docid = 48192
2 qid:1 1:0.600000 2:0.600000 3:0.625000 4:0.646018 5:0 6:0 7:0 8:0.688486 9:0.607305 10:0.643878 11:0.670966 12:0.871984 13:0.598684 14:0.595940 15:0.604204 16:0.210526 17:0.479859 18:0.446428 19:0.458833 22:0 23:0.482018 24:0.191077 25:0.628155 26:0.793447 27:0.932070 28:0.822335 29:0.828047 30:0.838679 31:0.255814 32:0.546115 33:0.497532 34:0.511428 35:0 36:0 37:0 38:0.535936 39:0.234274 40:0.696441 41:0.844624 42:0.950913 43:0.873818 44:0.862361 45:0.900401 #docid = 111457
2 qid:16 1:0.750000 2:0.750000 3:0.500000 4:0.535837 5:0 6:0 7:0 8:0.617886 9:0.678440 10:0.829193 11:0.820538 12:0.933762 13:0.835391 14:0.779902 15:0.779902 16:0.741935 17:0.907462 18:1.000000 19:1.000000 22:0 23:1.000000 24:0.629936 25:1.000000 26:0.978155 27:0.992875 28:0.809396 29:0.859716 30:0.843703 31:0.787879 32:0.899428 33:0.236364 34:0.263176 35:0 36:0 37:0 38:0.308543 39:0.663472 40:0.522875 41:0.928220 42:0.976300 43:0.819351 44:0.665771 45:0.721354 #docid = 77551
0 qid:16 1:0.750000 2:0.750000 3:0.600000 4:0.633761 5:0 6:0 7:0 8:0.706452 9:0.678440 10:0.873445 11:0.870330 12:0.953490 13:0.844582 14:0.813557 15:0.813557 16:0.161290 17:0.450835 18:0.502415 19:0.513837 22:0 23:0.472731 24:0.118967 25:0.607601 26:0.782479 27:0.920868 28:0.761328 29:0.810871 30:0.776528 31:0.242424 32:0.586275 33:0.160000 34:0.181778 35:0 36:0 37:0 38:0.207651 39:0.191958 40:0.410028 41:0.896155 42:0.965114 43:0.855650 44:0.700766 45:0.788961 #docid = 336131
2 qid:16 1:0.750000 2:0.750000 3:0.333333 4:0.366240 5:0 6:0 7:0 8:0.450797 9:0.678440 10:0.733080 11:0.700338 12:0.880717 13:0.808769 14:0.705316 15:0.705316 16:0.225806 17:0.565873 18:0.494565 19:0.506309 22:0 23:0.530383 24:0.192726 25:0.697755 26:0.870634 27:0.955308 28:0.788789 29:0.815581 30:0.813474 31:0.303030 32:0.647322 33:0.136986 34:0.155759 35:0 36:0 37:0 38:0.192331 39:0.260761 40:0.402634 41:0.888923 42:0.962536 43:0.830289 44:0.655093 45:0.727717 #docid = 251501
2 qid:16 1:0.750000 2:0.750000 3:0.600000 4:0.633761 5:0 6:0 7:0 8:0.706452 9:0.678440 10:0.873445 11:0.870330 12:0.953490 13:0.844582 14:0.813557 15:0.813557 16:0.225806 17:0.491646 18:0.659418 19:0.666205 22:0 23:0.754670 24:0.218996 25:0.868227 26:0.854483 27:0.949267 28:0.726861 29:0.757956 30:0.743703 31:0.303030 32:0.576007 33:0.188679 34:0.211530 35:0 36:0 37:0 38:0.273955 39:0.287129 40:0.500657 41:0.875014 42:0.957519 43:0.780551 44:0.627236 45:0.691513 #docid = 313204
0 qid:16 1:0.250000 2:0.250000 3:0.333333 4:0.333333 5:0 6:0 7:0 8:0.420475 9:0.321560 10:0.509470 11:0.470802 12:0.747727 13:0.427688 14:0.307775 15:0.307775 16:0.387097 17:0.647494 18:0.797955 19:0.798963 22:0 23:0.938939 24:0.412261 25:0.969178 26:0.928624 27:0.976111 28:0.759386 29:0.792259 30:0.801996 31:0.393939 32:0.624655 33:0.183099 34:0.203851 35:0 36:0 37:0 38:0.284949 39:0.426499 40:0.508488 41:0.871787 42:0.956343 43:0.758581 44:0.603141 45:0.674360 #docid = 313208
1 qid:16 1:0.500000 2:0.500000 3:0.400000 4:0.422507 5:0 6:0 7:0 8:0.513811 9:0.509110 10:0.684891 11:0.653973 12:0.857779 13:0.644522 14:0.556965 15:0.556965 16:0.451613 17:0.799345 18:0.475971 19:0.486276 22:0 23:0.557767 24:0.431399 25:0.739061 26:0.901088 27:0.966400 28:0.778388 29:0.776773 30:0.810657 31:0.484848 32:0.793891 33:0.115942 34:0.131855 35:0 36:0 37:0 38:0.177914 39:0.463326 40:0.395941 41:0.870686 42:0.955941 43:0.787500 44:0.601501 45:0.695046 #docid = 336133
1 qid:16 1:0.750000 2:0.750000 3:0.333333 4:0.366240 5:0 6:0 7:0 8:0.450797 9:0.678440 10:0.733080 11:0.700338 12:0.880717 13:0.808769 14:0.705316 15:0.705316 16:0.516129 17:0.767690 18:0.745851 19:0.752084 22:0 23:0.753912 24:0.429575 25:0.837245 26:0.897450 27:0.965095 28:0.758176 29:0.793104 30:0.797509 31:0.575758 32:0.785281 33:0.179245 34:0.201321 35:0 36:0 37:0 38:0.237659 39:0.477625 40:0.444060 41:0.870525 42:0.955882 43:0.771917 44:0.619518 45:0.687124 #docid = 145824
0 qid:16 1:0.500000 2:0.500000 3:0.250000 4:0.272947 5:0 6:0 7:0 8:0.363075 9:0.509110 10:0.607252 11:0.553255 12:0.801771 13:0.617429 14:0.498964 15:0.498964 16:0.258065 17:0.623392 18:0.482320 19:0.494334 22:0 23:0.492696 24:0.207796 25:0.651910 26:0.864017 27:0.952847 28:0.797777 29:0.815429 30:0.836508 31:0.303030 32:0.663036 33:0.120482 34:0.137461 35:0 36:0 37:0 38:0.166256 39:0.253132 40:0.366774 41:0.865708 42:0.954116 43:0.820995 44:0.639551 45:0.732058 #docid = 336132
2 qid:16 1:1.000000 2:1.000000 3:0.235294 4:0.264915 5:0 6:0 7:0 8:0.324758 9:0.828209 10:0.670373 11:0.613755 12:0.836524 13:0.908513 14:0.747990 15:0.747990 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.121212 32:0.369898 33:0.235294 34:0.264915 35:0 36:0 37:0 38:0.295786 39:0.095979 40:0.505119 41:0.863260 42:0.953216 43:0.819976 44:0.780842 45:0.795777 #docid = 178662
0 qid:60 1:1.000000 2:0.896241 3:0.727272 4:0.841200 5:0 6:0 7:0 8:0.924926 9:0.980298 10:0.851239 11:0.969811 12:0.988820 13:0.747556 14:0.780031 15:0.799822 16:0.571429 17:0.725168 18:0.627118 19:0.630238 22:0 23:0.747532 24:0.650173 25:0.858825 26:0.861109 27:0.953575 28:0.828263 29:0.776008 30:0.775743 31:0.750000 32:0.877600 33:0.428572 34:0.454793 35:0 36:0 37:0 38:0.638866 39:0.822460 40:0.878710 41:0.967048 42:0.989622 43:0.980087 44:0.914101 45:0.940860 #docid = 62992
0 qid:60 1:1.000000 2:1.000000 3:0.500000 4:0.598074 5:0 6:0 7:0 8:0.726738 9:0.951552 10:1.000000 11:0.931878 12:0.974268 13:1.000000 14:1.000000 15:1.000000 16:0.642857 17:0.910590 18:0.285104 19:0.292694 22:0 23:0.285050 24:0.537970 25:0.460998 26:0.672353 27:0.876755 28:0.831599 29:0.696125 30:0.739615 31:0.812500 32:1.000000 33:0.200617 34:0.217654 35:0 36:0 37:0 38:0.269467 39:0.701054 40:0.510132 41:0.789301 42:0.926722 43:0.903224 44:0.767954 45:0.841676 #docid = 117936
0 qid:60 1:0.500000 2:0.500000 3:0.444444 4:0.519702 5:0 6:0 7:0 8:0.559435 9:0.451552 10:0.561602 11:0.592322 12:0.808998 13:0.444275 14:0.462997 15:0.462997 16:0.928571 17:0.947485 18:0.556712 19:0.561783 22:0 23:0.637541 24:1.000000 25:0.755801 26:0.790423 27:0.926983 28:0.726017 29:0.655490 30:0.669611 31:0.937500 32:0.896205 33:0.320513 34:0.342099 35:0 36:0 37:0 38:0.477745 39:1.000000 40:0.725452 41:0.781924 42:0.923814 43:0.733690 44:0.665703 45:0.681064 #docid = 207282
0 qid:60 1:0.750000 2:0.750000 3:0.750000 4:0.871466 5:0 6:0 7:0 8:0.976529 9:0.724077 10:0.960545 11:1.000000 12:1.000000 13:0.785851 14:0.859230 15:0.859230 16:0.714286 17:0.814578 18:0.680149 19:0.682190 22:0 23:0.524118 24:0.480588 25:0.590625 26:0.673447 27:0.877260 28:0.734986 29:0.674166 30:0.688944 31:0.812500 32:0.845405 33:0.427633 34:0.453529 35:0 36:0 37:0 38:0.466898 39:0.596478 40:0.667148 41:0.752777 42:0.912048 43:0.779705 44:0.720773 45:0.751091 #docid = 59794
0 qid:60 1:0.500000 2:0.500000 3:0.444444 4:0.519702 5:0 6:0 7:0 8:0.687667 9:0.573795 10:0.795615 11:0.755622 12:0.897803 13:0.646004 14:0.631561 15:0.631561 16:0.857143 17:0.896628 18:0.420455 19:0.427438 22:0 23:0.492713 24:0.892407 25:0.688471 26:0.736582 27:0.905081 28:0.664113 29:0.597646 30:0.597186 31:0.875000 32:0.833221 33:0.248227 34:0.266540 35:0 36:0 37:0 38:0.392089 39:0.943282 40:0.684541 41:0.741280 42:0.907282 43:0.664297 44:0.602377 45:0.600192 #docid = 53675
0 qid:60 1:0.250000 2:0.250000 3:0.200000 4:0.235063 5:0 6:0 7:0 8:0.329095 9:0.301270 10:0.418131 11:0.378301 12:0.645475 13:0.304524 14:0.251136 15:0.251136 16:0.714286 17:0.814578 18:0.420455 19:0.426905 22:0 23:0.508589 24:0.784356 25:0.690816 26:0.732341 27:0.903288 28:0.664545 29:0.597012 30:0.618314 31:0.687500 32:0.734401 33:0.229168 34:0.245958 35:0 36:0 37:0 38:0.375201 39:0.782115 40:0.657805 41:0.716382 42:0.896701 43:0.650098 44:0.585744 45:0.604149 #docid = 59961
0 qid:60 1:0.000000 2:0.000000 3:0.000000 4:0.000000 5:0 6:0 7:0 8:0.000000 9:0.000000 10:0.000000 11:0.000000 12:0.000000 13:0.000000 14:0.000000 15:0.000000 16:0.500000 17:0.688273 18:0.483211 19:0.489930 22:0 23:0.563386 24:0.533364 25:0.722206 26:0.746229 27:0.909120 28:0.692902 29:0.634233 30:0.634796 31:0.437500 32:0.600604 33:0.233333 34:0.250991 35:0 36:0 37:0 38:0.369760 39:0.475902 40:0.652214 41:0.711077 42:0.894399 43:0.667020 44:0.607633 45:0.608534 #docid = 282649
0 qid:60 1:0.500000 2:0.500000 3:0.500000 4:0.580977 5:0 6:0 7:0 8:0.752944 9:0.573795 10:0.818634 11:0.794413 12:0.916061 13:0.651690 14:0.646858 15:0.646858 16:0.357143 17:0.502850 18:0.502720 19:0.506816 22:0 23:0.618531 24:0.409853 25:0.771663 26:0.688211 27:0.883992 28:0.596576 29:0.549403 30:0.563468 31:0.437500 32:0.529005 33:0.324075 34:0.344244 35:0 36:0 37:0 38:0.523551 39:0.509529 40:0.797060 41:0.710268 42:0.894046 43:0.615253 44:0.570477 45:0.573577 #docid = 162105
0 qid:60 1:0.250000 2:0.250000 3:0.500000 4:0.550341 5:0 6:0 7:0 8:0.634330 9:0.301270 10:0.508428 11:0.524936 12:0.764950 13:0.339568 14:0.310700 15:0.310700 16:0.428571 17:0.606222 18:0.330359 19:0.336422 22:0 23:0.423612 24:0.491312 25:0.635019 26:0.701840 27:0.890081 28:0.717360 29:0.649078 30:0.639140 31:0.437500 32:0.561200 33:0.198863 34:0.213529 35:0 36:0 37:0 38:0.341288 39:0.520403 40:0.622737 41:0.706395 42:0.892353 43:0.713440 44:0.647906 45:0.637388 #docid = 237130
0 qid:60 1:0.250000 2:0.250000 3:0.285714 4:0.329328 5:0 6:0 7:0 8:0.431643 9:0.301270 10:0.453182 11:0.439716 12:0.700342 13:0.321585 14:0.274313 15:0.274313 16:0.642857 17:0.807218 18:0.378409 19:0.385790 22:0 23:0.435240 24:0.656875 25:0.624906 26:0.704954 27:0.891455 28:0.667007 29:0.595384 30:0.627227 31:0.625000 32:0.731620 33:0.213675 34:0.230115 35:0 36:0 37:0 38:0.333337 39:0.666667 40:0.603632 41:0.697205 42:0.888297 43:0.658572 44:0.589585 45:0.620499 #docid = 311845
0 qid:60 1:0.250000 2:0.250000 3:1.000000 4:1.000000 5:0 6:0 7:0 8:0.951157 9:0.301270 10:0.577113 11:0.602823 12:0.815407 13:0.352121 14:0.355790 15:0.355790 16:0.357143 17:0.561066 18:0.420455 19:0.427289 22:0 23:0.486631 24:0.374504 25:0.657098 26:0.685184 27:0.882624 28:0.658340 29:0.607255 30:0.601431 31:0.375000 32:0.529005 33:0.263157 34:0.281388 35:0 36:0 37:0 38:0.409769 39:0.415155 40:0.664461 41:0.689648 42:0.884922 43:0.662972 44:0.613352 45:0.607515 #docid = 69468
0 qid:60 1:0.500000 2:0.500000 3:0.363636 4:0.429194 5:0 6:0 7:0 8:0.474869 9:0.451552 10:0.527597 11:0.539623 12:0.775014 13:0.433187 14:0.437517 15:0.437517 16:0.214286 17:0.362584 18:0.462500 19:0.466501 22:0 23:0.492269 24:0.204242 25:0.623835 26:0.564872 27:0.822677 28:0.531099 29:0.490427 30:0.489280 31:0.312500 32:0.510400 33:0.304877 34:0.327317 35:0 36:0 37:0 38:0.417938 39:0.291857 40:0.632786 41:0.687626 42:0.884013 43:0.659126 44:0.628465 45:0.629234 #docid = 205715
0 qid:60 1:0.750000 2:0.646240 3:0.461538 4:0.535703 5:0 6:0 7:0 8:0.700756 9:0.875066 10:0.792131 11:0.788099 12:0.913150 13:0.681428 14:0.628870 15:0.644786 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.187500 32:0.316400 33:0.576923 34:0.595676 35:0 36:0 37:0 38:0.844126 39:0.224391 40:0.986874 41:0.684789 42:0.882733 43:0.629400 44:0.639959 45:0.629068 #docid = 253630
0 qid:60 1:0.250000 2:0.250000 3:0.500000 4:0.550341 5:0 6:0 7:0 8:0.634330 9:0.301270 10:0.508428 11:0.524936 12:0.764950 13:0.339568 14:0.310700 15:0.310700 16:0.285714 17:0.502850 18:0.385416 19:0.393325 22:0 23:0.432654 24:0.282371 25:0.624956 26:0.664049 27:0.872897 28:0.650093 29:0.606123 30:0.598806 31:0.312500 32:0.489601 33:0.240385 34:0.258296 35:0 36:0 37:0 38:0.370605 39:0.333333 40:0.637667 41:0.678523 42:0.879885 43:0.656906 44:0.611545 45:0.607830 #docid = 173630
2 qid:63 1:0.142857 2:0.162076 3:0.075000 4:0.082826 5:0 6:0 7:0 8:0.117511 9:0.162051 10:0.201826 11:0.187453 12:0.518187 13:0.131888 14:0.100150 15:0.100150 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.026316 32:0.063236 33:0.133333 34:0.134867 35:0 36:0 37:0 38:0.185062 39:0.036628 40:0.256041 41:0.218264 42:0.600337 43:0.270977 44:0.088720 45:0.130623 #docid = 109188
2 qid:63 1:0.142857 2:0.162076 3:0.062500 4:0.069559 5:0 6:0 7:0 8:0.101651 9:0.162051 10:0.193517 11:0.171485 12:0.492566 13:0.119540 14:0.094148 15:0.074058 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.026316 32:0.063236 33:0.111111 34:0.113263 35:0 36:0 37:0 38:0.159870 39:0.036628 40:0.241940 41:0.214674 42:0.595982 43:0.263450 44:0.082976 45:0.106878 #docid = 191681
0 qid:63 1:0.050000 2:0.000000 3:0.000000 4:0.000000 5:0 6:0 7:0 8:0.000000 9:0.000000 10:0.000000 11:0.000000 12:0.000000 13:0.000000 14:0.000000 15:0.000000 16:0.108108 17:0.202925 18:0.133118 19:0.135694 22:0 23:0.122420 24:0.089676 25:0.188131 26:0.218182 27:0.599543 28:0.161125 29:0.132241 30:0.150754 31:0.105263 32:0.200453 33:0.053872 34:0.056603 35:0 36:0 37:0 38:0.054349 39:0.086185 40:0.107989 41:0.205438 42:0.584435 43:0.138906 44:0.081630 45:0.072691 #docid = 123928
0 qid:63 1:0.285714 2:0.324153 3:0.115385 4:0.128802 5:0 6:0 7:0 8:0.158860 9:0.260811 10:0.262112 11:0.264551 12:0.617330 13:0.218475 14:0.208065 15:0.208065 16:0.054054 17:0.101463 18:0.045759 19:0.046807 22:0 23:0.052486 24:0.055102 25:0.069303 26:0.118084 27:0.438054 28:0.000000 29:0.015419 30:0.000000 31:0.105263 32:0.189708 33:0.037825 34:0.039792 35:0 36:0 37:0 38:0.046696 39:0.105125 40:0.064656 41:0.204294 42:0.582969 43:0.088132 44:0.065962 45:0.031834 #docid = 105261
0 qid:63 1:0.142857 2:0.162076 3:0.187500 4:0.193916 5:0 6:0 7:0 8:0.219964 9:0.155887 10:0.232754 11:0.250211 12:0.601293 13:0.160330 14:0.122397 15:0.122397 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.026316 32:0.063236 33:0.333333 34:0.315756 35:0 36:0 37:0 38:0.325673 39:0.031733 40:0.281042 41:0.198997 42:0.576071 43:0.278119 44:0.098801 45:0.140219 #docid = 186807
0 qid:63 1:0.142857 2:0.162076 3:0.068182 4:0.075614 5:0 6:0 7:0 8:0.085071 9:0.119610 10:0.118424 11:0.132055 12:0.417374 13:0.043848 14:0.038707 15:0.038707 16:0.054054 17:0.128032 18:0.059766 19:0.061302 22:0 23:0.040034 24:0.031483 25:0.017187 26:0.100844 27:0.396541 28:0.034626 29:0.000000 30:0.006208 31:0.078947 32:0.189708 33:0.036697 34:0.038768 35:0 36:0 37:0 38:0.031501 39:0.053417 40:0.022600 41:0.170349 42:0.535255 43:0.111149 44:0.043010 45:0.026489 #docid = 50094
0 qid:63 1:0.142857 2:0.162076 3:0.125000 4:0.133960 5:0 6:0 7:0 8:0.137753 9:0.119610 10:0.144789 11:0.169829 12:0.489773 13:0.076053 14:0.058539 15:0.058539 16:0.054054 17:0.128032 18:0.075092 19:0.076919 22:0 23:0.056010 24:0.035293 25:0.036354 26:0.126655 27:0.456486 28:0.087814 29:0.036038 30:0.069390 31:0.078947 32:0.163463 33:0.047619 34:0.050043 35:0 36:0 37:0 38:0.042557 39:0.056934 40:0.034251 41:0.154984 42:0.510433 43:0.092318 44:0.014949 45:0.032874 #docid = 257182
0 qid:63 1:0.142857 2:0.162076 3:0.083333 4:0.091560 5:0 6:0 7:0 8:0.127506 9:0.162051 10:0.206637 11:0.196606 12:0.531907 13:0.138217 14:0.103619 15:0.103619 16:0.027027 17:0.064016 18:0.019018 19:0.019542 22:0 23:0.031368 24:0.039042 25:0.130170 26:0.100227 27:0.394927 28:0.000492 29:0.045364 30:0.033253 31:0.052632 32:0.100227 33:0.016360 34:0.017256 35:0 36:0 37:0 38:0.028239 39:0.073256 40:0.107416 41:0.146286 42:0.495268 43:0.000000 44:0.022711 45:0.004782 #docid = 339345
0 qid:63 1:0.142857 2:0.162076 3:0.250000 4:0.250000 5:0 6:0 7:0 8:0.167856 9:0.078385 10:0.103707 11:0.132891 12:0.419191 13:0.027399 14:0.027300 15:0.027300 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.026316 32:0.063236 33:0.444444 34:0.407079 35:0 36:0 37:0 38:0.189973 39:0.011360 40:0.068814 41:0.066431 42:0.287984 43:0.188175 44:0.000000 45:0.046177 #docid = 173315
2 qid:63 1:0.050000 2:0.000000 3:0.000000 4:0.000000 5:0 6:0 7:0 8:0.000000 9:0.000000 10:0.000000 11:0.000000 12:0.000000 13:0.000000 14:0.000000 15:0.000000 16:0.000000 17:0.000000 18:0.000000 19:0.000000 22:0 23:0.000000 24:0.000000 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.000000 30:0.000000 31:0.000000 32:0.000000 33:0.000000 34:0.000000 35:0 36:0 37:0 38:0.000000 39:0.000000 40:0.000000 41:0.000000 42:0.000000 43:0.000000 44:0.000000 45:0.000000 #docid = 9897
"""


def write_dataset_file(handle):
    handle.write(dataset_txt.encode('utf-8'))
    handle.seek(0)


def test_basic():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file)

    # Check data set size.
    assert len(dataset) == 4

    # Get first sample.
    sample = dataset[0]
    x, y, q = sample['features'], sample['relevance'], sample['qid']
    assert x.shape == (6, 45)
    assert y.shape == (6,)
    assert x[1, 2] == 1.0
    assert y[1] == 2.0
    assert q == 1

    # Get second sample.
    sample = dataset[1]
    x, y, q = sample['features'], sample['relevance'], sample['qid']
    assert x.shape == (9, 45)
    assert y.shape == (9,)
    assert float(x[5, 3]) == approx(0.422507)
    assert y[5] == 1.0
    assert q == 16

    # Get third sample.
    sample = dataset[2]
    x, y, q = sample['features'], sample['relevance'], sample['qid']
    assert x.shape == (14, 45)
    assert y.shape == (14,)
    assert float(x[12, 2]) == approx(0.461538)
    assert y[12] == 0.0
    assert q == 60

    # Get fourth sample.
    sample = dataset[3]
    x, y, q = sample['features'], sample['relevance'], sample['qid']
    assert x.shape == (10, 45)
    assert y.shape == (10,)
    assert float(x[8, 2]) == approx(0.25)
    assert y[8] == 0.0
    assert q == 63


def test_sparse():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset_sparse = load(file, sparse=True)

    with BytesIO() as file:
        write_dataset_file(file)
        dataset_dense = load(file, sparse=False)

    # Check data set size.
    assert len(dataset_dense) == len(dataset_sparse)

    # Check sparse and dense return same samples.
    for i in range(len(dataset_dense)):
        sample_dense = dataset_dense[i]
        sample_sparse = dataset_sparse[i]
        assert sample_sparse['qid'] == sample_dense['qid']
        assert sample_sparse['n'] == sample_dense['n']
        assert sample_sparse['features'].to_dense().numpy() == approx(
            sample_dense['features'].numpy())
        assert sample_sparse['relevance'].numpy() == approx(
            sample_dense['relevance'].numpy())


def test_normalize():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, normalize=True)

    # Check data set size.
    assert len(dataset) == 4

    # Get first sample and assert the contents is as expected.
    sample = dataset[0]
    x, y, q = sample['features'], sample['relevance'], sample['qid']
    assert x.shape == (6, 45)
    assert y.shape == (6,)
    assert q == 1

    assert float(x[0, 1]) == approx(1.0)
    assert float(x[1, 1]) == approx(0.5)
    assert float(x[2, 1]) == approx(0.25)
    assert float(x[3, 1]) == approx(0.0)
    assert float(x[4, 1]) == approx(0.125)
    assert float(x[5, 1]) == approx(0.5)

    assert float(x[0, 0]) == approx(0.24242424242424246)
    assert float(x[1, 0]) == approx(0.12121212121212122)
    assert float(x[2, 0]) == approx(0.060606060606060615)
    assert float(x[3, 0]) == approx(0.0)
    assert float(x[4, 0]) == approx(1.0)
    assert float(x[5, 0]) == approx(0.12121212121212122)


def test_sparse_normalize():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)

        # This should raise an error as it is not implemented.
        with raises(NotImplementedError):
            dataset = load(file, sparse=True, normalize=True)


def test_serialize():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, normalize=True)

    # Attempt to serialize and deserialize it.
    serialized = pickle.dumps(dataset)
    deserialized = pickle.loads(serialized)

    # Assert original and deserialized versions are the same.
    assert len(dataset) == len(deserialized)
    for i in range(len(dataset)):
        sample1 = dataset[i]
        x1, y1, q1 = sample1['features'], sample1['relevance'], sample1['qid']
        sample2 = deserialized[i]
        x2, y2, q2 = sample2['features'], sample2['relevance'], sample2['qid']
        assert x1.numpy() == approx(x2.numpy())
        assert y1.numpy() == approx(y2.numpy())
        assert q1 == q2


def test_serialize_sparse():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=True)

    # Attempt to serialize and deserialize it.
    serialized = pickle.dumps(dataset)
    deserialized = pickle.loads(serialized)

    # Assert original and deserialized versions are the same.
    assert len(dataset) == len(deserialized)
    for i in range(len(dataset)):
        sample1 = dataset[i]
        x1, y1, q1 = sample1['features'], sample1['relevance'], sample1['qid']
        sample2 = deserialized[i]
        x2, y2, q2 = sample2['features'], sample2['relevance'], sample2['qid']
        assert x1.to_dense().numpy() == approx(x2.to_dense().numpy())
        assert y1.numpy() == approx(y2.numpy())
        assert q1 == q2


def test_double_serialize():
    # Load data set
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, normalize=True)

    # Attempt to serialize and deserialize it multiple times.
    s1 = pickle.dumps(dataset)
    d1 = pickle.loads(s1)
    s2 = pickle.dumps(d1)
    deserialized = pickle.loads(s2)

    # Assert original and deserialized versions are the same.
    assert len(dataset) == len(deserialized)
    for i in range(len(dataset)):
        sample1 = dataset[i]
        x1, y1, q1 = sample1['features'], sample1['relevance'], sample1['qid']
        sample2 = deserialized[i]
        x2, y2, q2 = sample2['features'], sample2['relevance'], sample2['qid']
        assert x1.numpy() == approx(x2.numpy())
        assert y1.numpy() == approx(y2.numpy())
        assert q1 == q2


def test_collate_sparse_10():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=True)

    # Construct a batch of three samples and collate it with a maximum list
    # size of 10.
    batch = [dataset[0], dataset[1], dataset[2]]
    collate_fn = create_svmranking_collate_fn(max_list_size=10)

    # Assert resulting tensor shape is as expected.
    tensor_batch = collate_fn(batch)
    assert tensor_batch["features"].shape == (3, 10, 45)


def test_collate_dense_10():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=False)

    # Construct a batch of three samples and collate it with a maximum list
    # size of 10.
    batch = [dataset[0], dataset[1], dataset[2]]
    collate_fn = create_svmranking_collate_fn(max_list_size=10)

    # Assert resulting tensor shape is as expected.
    tensor_batch = collate_fn(batch)
    assert tensor_batch["features"].shape == (3, 10, 45)


def test_collate_sparse_3():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=True)

    # Construct a batch of three samples and collate it with a maximum list
    # size of 3.
    batch = [dataset[0], dataset[1], dataset[2]]
    collate_fn = create_svmranking_collate_fn(max_list_size=3)

    # Assert resulting tensor shape is as expected.
    tensor_batch = collate_fn(batch)
    assert tensor_batch["features"].shape == (3, 3, 45)


def test_collate_dense_3():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=False)

    # Construct a batch of three samples and collate it with a maximum list
    # size of 3.
    batch = [dataset[0], dataset[1], dataset[2]]
    collate_fn = create_svmranking_collate_fn(max_list_size=3)

    # Assert resulting tensor shape is as expected.
    tensor_batch = collate_fn(batch)
    assert tensor_batch["features"].shape == (3, 3, 45)


def test_collate_sparse_all():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=True)

    # Construct a batch of three samples and collate it with an unlimited
    # maximum list size.
    batch = [dataset[0], dataset[1], dataset[2]]
    collate_fn = create_svmranking_collate_fn(max_list_size=None)

    # Assert resulting tensor shape is as expected.
    tensor_batch = collate_fn(batch)
    assert tensor_batch["features"].shape == (3, 14, 45)


def test_collate_dense_all():

    # Load data set.
    with BytesIO() as file:
        write_dataset_file(file)
        dataset = load(file, sparse=False)

    # Construct a batch of three samples and collate it with an unlimited
    # maximum list size.
    batch = [dataset[0], dataset[1], dataset[2]]
    collate_fn = create_svmranking_collate_fn(max_list_size=None)

    # Assert resulting tensor shape is as expected.
    tensor_batch = collate_fn(batch)
    assert tensor_batch["features"].shape == (3, 14, 45)
